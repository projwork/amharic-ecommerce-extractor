{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Amharic E-commerce Data Extractor\n",
    "\n",
    "This notebook demonstrates the data ingestion and preprocessing pipeline for Ethiopian Telegram e-commerce channels.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project implements a comprehensive data ingestion system for scraping and processing messages from Ethiopian Telegram e-commerce channels. The system includes:\n",
    "\n",
    "1. **Data Ingestion**: Automated scraping from multiple Telegram channels with rate limiting\n",
    "2. **Data Preprocessing**: Amharic text processing, entity extraction, and data cleaning\n",
    "3. **Feature Engineering**: E-commerce specific feature extraction and analysis\n",
    "4. **Data Quality Assessment**: Comprehensive quality reports and metrics\n",
    "\n",
    "## Channels Analyzed\n",
    "\n",
    "The system scrapes data from the following Ethiopian e-commerce channels:\n",
    "- @sinayelj\n",
    "- @Shewabrand\n",
    "- @helloomarketethiopia\n",
    "- @modernshoppingcenter\n",
    "- @qnashcom\n",
    "- @Shageronlinestore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to path for imports\n",
    "sys.path.append(str(Path('../src')))\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_ingestion import TelegramDataIngestion, ETHIOPIAN_ECOMMERCE_CHANNELS\n",
    "from src.data_preprocessing import EcommerceDataPreprocessor, AmharicTextPreprocessor\n",
    "from src.config import PATHS\n",
    "from src.utils import load_json_file\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ“Š Channels to analyze: {len(ETHIOPIAN_ECOMMERCE_CHANNELS)}\")\n",
    "print(f\"ğŸ“ Data directory: {PATHS['data_dir']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Data Ingestion Demo\n",
    "\n",
    "Before running the actual scraping, let's demonstrate the Amharic text processing capabilities with sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Amharic Text Processing\n",
    "amharic_processor = AmharicTextPreprocessor()\n",
    "\n",
    "# Sample Ethiopian e-commerce messages\n",
    "sample_messages = [\n",
    "    \"áŠ á‹²áˆµ áˆµáˆáŠ­ áˆˆáˆ½á‹«áŒ­ á‹‹áŒ‹ 15000 á‰¥áˆ­ á‰ áŒ£áˆ áŒ¥áˆ«á‰µ á‹«áˆˆá‹ áˆ›áŒáŠ˜á‰µ á‹­á‰»áˆ‹áˆ @mystore\",\n",
    "    \"New iPhone 13 for sale, price 25000 ETB, excellent condition! Contact +251911234567\",\n",
    "    \"áˆ½á‹«áŒ­ á‰ áˆ­áŠ«á‰³ á‹•á‰ƒá‹á‰½ áŠ áˆ‰ á‹‹áŒ‹ á‰°áˆ˜áŒ£áŒ£áŠ áŠá‹á£ áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ @shopethiopia á‹­áŒ á‹­á‰\",\n",
    "    \"Ladies shoes collection áˆˆáˆ´á‰¶á‰½ áŒ«áˆ› á‰…áŠ“áˆ½ 30% off! á‹‹áŒ‹ 2000-5000 á‰¥áˆ­ á‹­áŒ€áˆáˆ«áˆ\",\n",
    "    \"Electronics store á‰ áŠ á‹²áˆµ áŠ á‰ á‰£ áˆ˜áŒ‹á‹áŠ• áŠ®áˆá’á‹©á‰°áˆ®á‰½á£ áˆµáˆáŠ®á‰½ áŠ¥áŠ“ á‰°á‹›áˆ›áŒ… á‹•á‰ƒá‹á‰½ áŠ áˆ‰\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ”¤ AMHARIC TEXT PROCESSING DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(sample_messages, 1):\n",
    "    print(f\"\\nğŸ“ Sample Message {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Is Amharic: {amharic_processor.is_amharic_text(text)}\")\n",
    "    print(f\"Normalized: {amharic_processor.normalize_amharic_text(text)}\")\n",
    "    \n",
    "    # Extract prices\n",
    "    prices = amharic_processor.extract_prices(text)\n",
    "    if prices:\n",
    "        print(f\"ğŸ’° Prices found: {[p['value'] for p in prices]} ETB\")\n",
    "    \n",
    "    # Extract contact info\n",
    "    contact = amharic_processor.extract_contact_info(text)\n",
    "    if contact['phone_numbers'] or contact['telegram_usernames']:\n",
    "        print(f\"ğŸ“ Contact info: {contact}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = amharic_processor.tokenize_amharic(text)\n",
    "    print(f\"ğŸ”¤ Tokens: {tokens[:10]}...\")  # Show first 10 tokens\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Analysis\n",
    "\n",
    "If you have scraped data available, this section will analyze it. Otherwise, it will show how to work with the processed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing scraped data\n",
    "raw_data_dir = PATHS['raw_data_dir']\n",
    "processed_data_dir = PATHS['processed_data_dir']\n",
    "\n",
    "# Look for combined data files\n",
    "combined_files = list(raw_data_dir.glob(\"combined_data_*.json\"))\n",
    "processed_files = list(processed_data_dir.glob(\"processed_*.csv\"))\n",
    "\n",
    "print(f\"ğŸ“Š DATA AVAILABILITY CHECK\")\n",
    "print(f\"Raw data files found: {len(combined_files)}\")\n",
    "print(f\"Processed data files found: {len(processed_files)}\")\n",
    "\n",
    "if combined_files:\n",
    "    print(f\"\\nğŸ“ Latest raw data file: {combined_files[-1].name}\")\n",
    "elif processed_files:\n",
    "    print(f\"\\nğŸ“ Latest processed data file: {processed_files[-1].name}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No scraped data found. To scrape data, run:\")\n",
    "    print(f\"   python scripts/telegram_scrapper.py\")\n",
    "    print(f\"   or\")\n",
    "    print(f\"   python scripts/run_data_ingestion.py\")\n",
    "\n",
    "# Create sample data if no real data is available\n",
    "if not combined_files and not processed_files:\n",
    "    print(f\"\\nğŸ¯ Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Generate sample data\n",
    "    sample_data = []\n",
    "    for i, channel in enumerate(ETHIOPIAN_ECOMMERCE_CHANNELS):\n",
    "        for j in range(10):  # 10 messages per channel\n",
    "            sample_data.append({\n",
    "                'message_id': f\"{i}_{j}\",\n",
    "                'channel_title': f\"Sample Channel {i+1}\",\n",
    "                'channel_username': channel,\n",
    "                'text': f\"Sample message {j+1} from {channel}\",\n",
    "                'date': datetime.now().isoformat(),\n",
    "                'views': np.random.randint(10, 1000),\n",
    "                'forwards': np.random.randint(0, 50),\n",
    "                'replies': np.random.randint(0, 20),\n",
    "                'has_media': np.random.choice([True, False]),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    df_sample = pd.DataFrame(sample_data)\n",
    "    print(f\"âœ… Sample dataset created with {len(df_sample)} messages\")\n",
    "    \n",
    "else:\n",
    "    # Load actual data if available\n",
    "    if combined_files:\n",
    "        latest_file = max(combined_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"\\nğŸ“Š Loading data from: {latest_file.name}\")\n",
    "        \n",
    "        with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        df_sample = pd.DataFrame(data)\n",
    "        \n",
    "    elif processed_files:\n",
    "        latest_file = max(processed_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"\\nğŸ“Š Loading processed data from: {latest_file.name}\")\n",
    "        df_sample = pd.read_csv(latest_file)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Dataset shape: {df_sample.shape}\")\n",
    "print(f\"ğŸ” Columns: {list(df_sample.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
